CIFAR-10 Experiments
Experiment with some hyperparameters and architectures and draw insights from the results. Some hyperparameters we will play with are:

Adding and removing dropouts in convolutional layers

Batch Normalization (BN)

L2 regularisation

Increasing the number of convolution layers

Increasing the number of filters in certain layers 

 

Experiment - I: Using dropouts after conv and FC layers

In the first experiment, we will use dropouts both after the convolutional and fully connected layers. 

Dropouts After Conv and FC layers

Training accuracy =  84%, validation accuracy = 79%



Experiment - II: Remove the dropouts after the convolutional layers (but retain them in the FC layer). Also, use batch normalization after every convolutional layer.

 
Recall that batch normalisation (BN) normalises the outputs from each layer with the mean and standard deviation of the batch. 
You may quickly revisit the lectures on BN here.


Experiment - III: Use batch normalization and dropouts after every convolutional layer. Also, retain the dropouts in the FC layer.

 

Experiment - IV: Remove the dropouts after the convolutional layers and use L2 regularization in the FC layer. Retain the dropouts in FC.



Experiment - I (Use dropouts after conv and FC layers, no BN): 
Training accuracy =  84%, validation accuracy  =  79%
Experiment - II (Remove dropouts from conv layers, retain dropouts in FC, use BN): 
Training accuracy =  98%, validation accuracy  =  79%
Experiment - III (Use dropouts after conv and FC layers, use BN):
Training accuracy =  89%, validation accuracy  =  82%
Experiment - IV (Remove dropouts from conv layers, use L2 + dropouts in FC, use BN):
Training accuracy = 94%, validation accuracy = 76%. 


Experiment-V: Dropouts after conv layer, L2 in FC, use BN after convolutional layer

 

Experiment-VI: Add a new convolutional layer to the network. Note that by a 'convolutional layer', 
the professor is referring to a convolutional unit with two sets of Conv2D layers with 128 filters each 
(we are abusing the terminology a bit here). The code for the additional conv layer is shown below.


Experiment-V: Dropouts after conv layer, L2 in FC, use BN after convolutional layer

Train accuracy =  86%, validation accuracy = 83%

 

Experiment-VI: Add a new convolutional layer to the network

Train accuracy =  89%, validation accuracy = 84%






Experiment - VII: Add more feature maps to the conv layers: from 32 to 64 and 64 to 128.
Train accuracy =  92%, validation accuracy = 84%

